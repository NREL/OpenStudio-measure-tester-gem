{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8c865a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#require_relative 'lib/openstudio/bem_to_surrogate/config.rb'\n",
    "IRuby::Kernel.instance.switch_backend!(:pry)\n",
    "$LOAD_PATH << File.expand_path('./lib') # Resolve global paths\n",
    "\n",
    "require 'json'\n",
    "require 'pathname'\n",
    "require 'yaml'\n",
    "require 'rexml'\n",
    "\n",
    "require 'openstudio_measure_tester'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "1e02bf58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(irb):1: warning: already initialized constant #<Class:0x0000556432993ec8>::JUnitType::MINITEST\n",
      "(irb):1: warning: previous definition of MINITEST was here\n",
      "(irb):2: warning: already initialized constant #<Class:0x0000556432993ec8>::JUnitType::PYTEST\n",
      "(irb):2: warning: previous definition of PYTEST was here\n",
      "(irb):3: warning: already initialized constant #<Class:0x0000556432993ec8>::JUnitType::ACCEPTED_VALUES\n",
      "(irb):3: warning: previous definition of ACCEPTED_VALUES was here\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       ":format_github_annotation"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module JUnitType\n",
    "  MINITEST = 'minitest'\n",
    "  PYTEST = 'pytest'\n",
    "  ACCEPTED_VALUES = [MINITEST, PYTEST]\n",
    "\n",
    "  def self.validate(v)\n",
    "    raise \"Unknow value #{v}: #{ACCEPTED_VALUES}\" unless ACCEPTED_VALUES.include?(v)\n",
    "  end\n",
    "end\n",
    "\n",
    "\n",
    "def parse_junit_xunit1(xml_path, junit_type:)\n",
    "\n",
    "  JUnitType.validate(junit_type)\n",
    "\n",
    "  doc = REXML::Document.new(File.open(xml_path)).root\n",
    "\n",
    "  if not doc\n",
    "    return nil\n",
    "  end\n",
    "\n",
    "  testsuite_element = doc.elements['testsuite']\n",
    "  errors, failures, skipped, annotations = parse_testsuite(testsuite_element, junit_type: junit_type)\n",
    "\n",
    "  result = {\n",
    "    #measure_compatibility_errors: json_data[:compatible] ? 0 1\n",
    "   measure_tests: testsuite_element.attributes['tests'].to_i,\n",
    "   measure_assertions: testsuite_element.attributes['assertions'].to_i,\n",
    "   measure_errors: testsuite_element.attributes['errors'].to_i,\n",
    "   measure_failures: testsuite_element.attributes['failures'].to_i,\n",
    "   measure_skipped: testsuite_element.attributes['skipped'].to_i,\n",
    "   issues: {\n",
    "     errors: errors,\n",
    "     failures: failures,\n",
    "     skipped: skipped,\n",
    "      # compatibility_error: json_data[:compatible] ? 0 1\n",
    "    },\n",
    "    annotations: annotations,\n",
    "  }\n",
    "\n",
    "  result\n",
    "end\n",
    "\n",
    "def parse_testsuite(testsuite_element, junit_type:)\n",
    "  errors = []\n",
    "  failures = []\n",
    "  skipped = []\n",
    "\n",
    "  JUnitType.validate(junit_type)\n",
    "\n",
    "  annotations = []\n",
    "  filepath = nil\n",
    "  if junit_type == JUnitType::MINITEST\n",
    "    filepath = testsuite_element.attributes['filepath']\n",
    "  end\n",
    "\n",
    "  testsuite_element.elements.each('testcase') do |testcase|\n",
    "    if testcase.elements['error']\n",
    "      errors << testcase.elements['error']\n",
    "    elsif testcase.elements['failure']\n",
    "      failures << testcase.elements['failure']\n",
    "    elsif testcase.elements['skipped']\n",
    "      skipped << 'Skipped test: ' + testcase.elements['skipped'].attributes['type']\n",
    "    end\n",
    "\n",
    "    annotations += prepare_testcase_annotations(testcase, junit_type: junit_type, filepath: filepath)\n",
    "  end\n",
    "\n",
    "  return errors, failures, skipped, annotations\n",
    "end\n",
    "\n",
    "def prepare_testcase_annotations(testcase, junit_type:, filepath: nil)\n",
    "\n",
    "  annotations = []\n",
    "\n",
    "  JUnitType.validate(junit_type)\n",
    "  is_minitest = (junit_type == JUnitType::MINITEST)\n",
    "  line_key = is_minitest ? 'lineno' : 'line'\n",
    "\n",
    "  raise \"When Minitest, filepath can't be nil\" if is_minitest && filepath.nil?\n",
    "\n",
    "\n",
    "  name = testcase.attributes['name']\n",
    "  line = testcase.attributes[line_key].to_i\n",
    "  classname = testcase.attributes['classname']\n",
    "\n",
    "  # annot = \"::error file=#{filepath},line=#{line},endLine=#{line + 1},title=#{title}::#{classname}.#{name}:#{message}\"\n",
    "  testcase.elements.each('failure') do |x|\n",
    "    title = x.attributes['type']\n",
    "    message = x.attributes['message']\n",
    "    if !is_minitest\n",
    "      filepath = testcase.attributes['file']\n",
    "    end\n",
    "    annotations << {\n",
    "      type: 'error',\n",
    "      classname: classname,\n",
    "      name: name,\n",
    "      file: filepath,\n",
    "      line: line,\n",
    "      title: title,\n",
    "      message: message,\n",
    "    }\n",
    "  end\n",
    "  testcase.elements.each('error') do |x|\n",
    "    title = x.attributes['type']\n",
    "    message = x.attributes['message']\n",
    "    if !is_minitest\n",
    "      filepath = testcase.attributes['file']\n",
    "    end\n",
    "    annotations << {\n",
    "      type: 'error',\n",
    "      classname: classname,\n",
    "      name: name,\n",
    "      file: filepath,\n",
    "      line: line,\n",
    "      title: title,\n",
    "      message: message,\n",
    "    }\n",
    "  end\n",
    "  testcase.elements.each('skipped') do |x|\n",
    "    title = x.attributes['type']\n",
    "    message = x.attributes['message']\n",
    "    if !is_minitest\n",
    "      filepath = testcase.attributes['file']\n",
    "    end\n",
    "    annotations << {\n",
    "      type: 'warning',\n",
    "      classname: classname,\n",
    "      name: name,\n",
    "      file: filepath,\n",
    "      line: line,\n",
    "      title: title,\n",
    "      message: message,\n",
    "    }\n",
    "  end\n",
    "\n",
    "  return annotations\n",
    "end\n",
    "\n",
    "def format_github_annotation(annotation_hash)\n",
    "  \"::#{annotation_hash[:type]} file=#{annotation_hash[:file]},line=#{annotation_hash[:line]},endLine=#{annotation_hash[:line] + 1},title=#{annotation_hash[:title]}::#{annotation_hash[:classname]}.#{annotation_hash[:name]}:#{annotation_hash[:message]}\"\n",
    "end"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e828e168",
   "metadata": {},
   "source": [
    "def parse_junit_xunit1(xml_path)\n",
    "\n",
    "  doc = REXML::Document.new(File.open(xml_path)).root\n",
    "\n",
    "  if not doc\n",
    "    return nil\n",
    "  end\n",
    "\n",
    "  testsuite_element = doc.elements['testsuite']\n",
    "  errors, failures, skipped = parse_testsuite(testsuite_element)\n",
    "  \n",
    "  result = {\n",
    "    #measure_compatibility_errors: json_data[:compatible] ? 0 1\n",
    "   measure_tests: testsuite_element.attributes['tests'].to_i,\n",
    "   measure_assertions: testsuite_element.attributes['assertions'].to_i,\n",
    "   measure_errors: testsuite_element.attributes['errors'].to_i,\n",
    "   measure_failures: testsuite_element.attributes['failures'].to_i,\n",
    "   measure_skipped: testsuite_element.attributes['skipped'].to_i,\n",
    "   issues: {\n",
    "     errors: errors,\n",
    "     failures: failures,\n",
    "     skipped: skipped,\n",
    "      # compatibility_error: json_data[:compatible] ? 0 1\n",
    "    }\n",
    "  }\n",
    "\n",
    "  result\n",
    "end\n",
    "\n",
    "def parse_testsuite(testsuite_element)\n",
    "  errors = []\n",
    "  failures = []\n",
    "  skipped = []\n",
    "\n",
    "  testsuite_element.elements.each('testcase') do |testcase|\n",
    "    if testcase.elements['error']\n",
    "      errors << testcase.elements['error']\n",
    "    elsif testcase.elements['failure']\n",
    "      failures << testcase.elements['failure']\n",
    "    elsif testcase.elements['skipped']\n",
    "      skipped << 'Skipped test: ' + testcase.elements['skipped'].attributes['type']\n",
    "    end\n",
    "  end\n",
    "\n",
    "  return errors, failures, skipped\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "17f37a57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_results = 'spec/files/pytest'\n",
    "@path_to_results = Pathname.new(path_to_results)\n",
    "@junit_xml = @path_to_results / 'junit.xml'\n",
    "@has_results = @path_to_results.directory?\n",
    "@has_results ||= @junit_xml.file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "255fb4a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "::error file=python_modelmeasure/tests/test_my_example_python_model_measure.py,line=18,endLine=19,title=::python_modelmeasure.tests.test_my_example_python_model_measure.TestMyExamplePythonModelMeasure.test_number_of_arguments_and_argument_names:assert 1 == 2\n"
     ]
    }
   ],
   "source": [
    "h = parse_junit_xunit1(@junit_xml, junit_type: JUnitType::PYTEST)\n",
    "puts format_github_annotation(h[:annotations][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "96fd8379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{:type=>\"error\", :classname=>\"python_modelmeasure.tests.test_my_example_python_model_measure.TestMyExamplePythonModelMeasure\", :name=>\"test_number_of_arguments_and_argument_names\", :file=>\"python_modelmeasure/tests/test_my_example_python_model_measure.py\", :line=>18, :title=>nil, :message=>\"assert 1 == 2\"}"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h[:annotations][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "9ab08c2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "::error file=spec/test_measures/AddOverhangsByProjectionFactor/tests/AddOverhangsByProjectionFactor_Test.rb,line=192,endLine=193,title=test_failure::AddOverhangsByProjectionFactor_Test.test_failure:RuntimeError: This will be an error...\n"
     ]
    }
   ],
   "source": [
    "h = parse_junit_xunit1('spec/files/minitest/reports/TEST-AddOverhangsByProjectionFactor-Test.xml',\n",
    "  junit_type: JUnitType::MINITEST)\n",
    "puts format_github_annotation(h[:annotations][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd064fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "format_github_annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d620046c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"measure_tests\": 0,\n",
      "  \"measure_assertions\": 0,\n",
      "  \"measure_errors\": 0,\n",
      "  \"measure_failures\": 0,\n",
      "  \"measure_skipped\": 0,\n",
      "  \"issues\": {\n",
      "    \"errors\": [\n",
      "\n",
      "    ],\n",
      "    \"failures\": [\n",
      "\n",
      "    ],\n",
      "    \"skipped\": [\n",
      "\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "  result = {}\n",
    "  # result[:measure_compatibility_errors] = json_data[:compatible] ? 0 : 1\n",
    "  result[:measure_tests] = 0\n",
    "  result[:measure_assertions] = 0\n",
    "  result[:measure_errors] = 0\n",
    "  result[:measure_failures] = 0\n",
    "  result[:measure_skipped] = 0\n",
    "  result[:issues] = {\n",
    "    errors: [],\n",
    "    failures: [],\n",
    "    skipped: [],\n",
    "    # compatibility_error: json_data[:compatible] ? 0 : 1\n",
    "  }\n",
    "puts JSON.pretty_generate result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "188c48f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<testsuites> ... </>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = REXML::Document.new(@junit_xml.read).root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c462d857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<testsuites>\n",
      "  <testsuite errors='0' failures='1' hostname='julien-desktop' name='pytest' skipped='0' tests='6' time='0.826' timestamp='2024-04-17T16:07:36.989631'>\n",
      "    <testcase classname='python_energyplusmeasure.tests.test_my_example_python_energyplus_measure.TestMyExamplePythonEnergyPlusMeasure' file='test_python_energyplusmeasure/tests/test_my_example_python_energyplus_measure.py' line='18' name='test_number_of_arguments_and_argument_names' time='0.003'/>\n",
      "    <testcase classname='python_energyplusmeasure.tests.test_my_example_python_energyplus_measure.TestMyExamplePythonEnergyPlusMeasure' file='test_python_energyplusmeasure/tests/test_my_example_python_energyplus_measure.py' line='33' name='test_bad_argument_values' time='0.004'/>\n",
      "    <testcase classname='python_energyplusmeasure.tests.test_my_example_python_energyplus_measure.TestMyExamplePythonEnergyPlusMeasure' file='test_python_energyplusmeasure/tests/test_my_example_python_energyplus_measure.py' line='62' name='test_good_argument_values' time='0.003'/>\n",
      "    <testcase classname='python_modelmeasure.tests.test_my_example_python_model_measure.TestMyExamplePythonModelMeasure' file='python_modelmeasure/tests/test_my_example_python_model_measure.py' line='18' name='test_number_of_arguments_and_argument_names' time='0.002'>\n",
      "      <failure message='assert 1 == 2'>self = &lt;test_my_example_python_model_measure.TestMyExamplePythonModelMeasure object at 0x7fc9c1145cd0&gt;\n",
      "\n",
      "    def test_number_of_arguments_and_argument_names(self):\n",
      "        \"\"\"Test that the arguments are what we expect.\"\"\"\n",
      "        # create an instance of the measure\n",
      "        measure = MyExamplePythonModelMeasure()\n",
      "    \n",
      "        # make an empty model\n",
      "        model = openstudio.model.Model()\n",
      "    \n",
      "        # get arguments and test that they are what we are expecting\n",
      "        arguments = measure.arguments(model)\n",
      "        num_args = arguments.size()\n",
      "&gt;       assert num_args == 2\n",
      "E       assert 1 == 2\n",
      "\n",
      "python_modelmeasure/tests/test_my_example_python_model_measure.py:30: AssertionError</failure>\n",
      "    </testcase>\n",
      "    <testcase classname='python_modelmeasure.tests.test_my_example_python_model_measure.TestMyExamplePythonModelMeasure' file='python_modelmeasure/tests/test_my_example_python_model_measure.py' line='32' name='test_bad_argument_values' time='0.001'/>\n",
      "    <testcase classname='python_modelmeasure.tests.test_my_example_python_model_measure.TestMyExamplePythonModelMeasure' file='python_modelmeasure/tests/test_my_example_python_model_measure.py' line='72' name='test_good_argument_values' time='0.062'/>\n",
      "  </testsuite>\n",
      "</testsuites>\n"
     ]
    }
   ],
   "source": [
    "puts doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "535bb114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<testcase classname='python_energyplusmeasure.tests.test_my_example_python_energyplus_measure.TestMyExamplePythonEnergyPlusMeasure' name='test_number_of_arguments_and_argument_names' file='test_python_energyplusmeasure/tests/test_my_example_python_energyplus_measure.py' line='18' time='0.003'/>, <testcase classname='python_energyplusmeasure.tests.test_my_example_python_energyplus_measure.TestMyExamplePythonEnergyPlusMeasure' name='test_bad_argument_values' file='test_python_energyplusmeasure/tests/test_my_example_python_energyplus_measure.py' line='33' time='0.004'/>, <testcase classname='python_energyplusmeasure.tests.test_my_example_python_energyplus_measure.TestMyExamplePythonEnergyPlusMeasure' name='test_good_argument_values' file='test_python_energyplusmeasure/tests/test_my_example_python_energyplus_measure.py' line='62' time='0.003'/>, <testcase classname='python_modelmeasure.tests.test_my_example_python_model_measure.TestMyExamplePythonModelMeasure' name='test_number_of_arguments_and_argument_names' file='python_modelmeasure/tests/test_my_example_python_model_measure.py' line='18' time='0.002'> ... </>, <testcase classname='python_modelmeasure.tests.test_my_example_python_model_measure.TestMyExamplePythonModelMeasure' name='test_bad_argument_values' file='python_modelmeasure/tests/test_my_example_python_model_measure.py' line='32' time='0.001'/>, <testcase classname='python_modelmeasure.tests.test_my_example_python_model_measure.TestMyExamplePythonModelMeasure' name='test_good_argument_values' file='python_modelmeasure/tests/test_my_example_python_model_measure.py' line='72' time='0.062'/>]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = nil\n",
    "all_annotations = []\n",
    "testsuite_element.elements.each('testcase') do |testcase|\n",
    "  # [\"classname\", \"name\", \"file\", \"line\", \"time\"]\n",
    "  classname = testcase.attributes['classname']\n",
    "  name = testcase.attributes['name']\n",
    "  filepath = testcase.attributes['file']\n",
    "  line = testcase.attributes['line'].to_i\n",
    "  testcase.elements.each('failure') do |x|\n",
    "    title = x.attributes['message']\n",
    "    message = x.text\n",
    "    \n",
    "    all_annotations << {\n",
    "      classname: classname,\n",
    "      name: name,\n",
    "      file: filepath,\n",
    "      line: line,\n",
    "      title: title,\n",
    "      # message: \"#{title}\\n\\n#{message}\",\n",
    "      message: message,\n",
    "    }\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "7e6c5867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"classname\": \"python_modelmeasure.tests.test_my_example_python_model_measure.TestMyExamplePythonModelMeasure\",\n",
      "  \"name\": \"test_number_of_arguments_and_argument_names\",\n",
      "  \"file\": \"python_modelmeasure/tests/test_my_example_python_model_measure.py\",\n",
      "  \"line\": 18,\n",
      "  \"title\": \"assert 1 == 2\",\n",
      "  \"message\": \"self = <test_my_example_python_model_measure.TestMyExamplePythonModelMeasure object at 0x7fc9c1145cd0>\\n\\n    def test_number_of_arguments_and_argument_names(self):\\n        \\\"\\\"\\\"Test that the arguments are what we expect.\\\"\\\"\\\"\\n        # create an instance of the measure\\n        measure = MyExamplePythonModelMeasure()\\n    \\n        # make an empty model\\n        model = openstudio.model.Model()\\n    \\n        # get arguments and test that they are what we are expecting\\n        arguments = measure.arguments(model)\\n        num_args = arguments.size()\\n>       assert num_args == 2\\nE       assert 1 == 2\\n\\npython_modelmeasure/tests/test_my_example_python_model_measure.py:30: AssertionError\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "puts JSON.pretty_generate all_annotations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "55e003ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assert 1 == 2\n",
      "\n",
      "self = <test_my_example_python_model_measure.TestMyExamplePythonModelMeasure object at 0x7fc9c1145cd0>\n",
      "\n",
      "    def test_number_of_arguments_and_argument_names(self):\n",
      "        \"\"\"Test that the arguments are what we expect.\"\"\"\n",
      "        # create an instance of the measure\n",
      "        measure = MyExamplePythonModelMeasure()\n",
      "    \n",
      "        # make an empty model\n",
      "        model = openstudio.model.Model()\n",
      "    \n",
      "        # get arguments and test that they are what we are expecting\n",
      "        arguments = measure.arguments(model)\n",
      "        num_args = arguments.size()\n",
      ">       assert num_args == 2\n",
      "E       assert 1 == 2\n",
      "\n",
      "python_modelmeasure/tests/test_my_example_python_model_measure.py:30: AssertionError\n"
     ]
    }
   ],
   "source": [
    "puts all_annotations[0][:message]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6e77b0f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"classname\", \"name\", \"file\", \"line\", \"time\"]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testcase.attributes.keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a986dc2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"self = <test_my_example_python_model_measure.TestMyExamplePythonModelMeasure object at 0x7fc9c1145cd0>\\n\\n    def test_number_of_arguments_and_argument_names(self):\\n        \\\"\\\"\\\"Test that the arguments are what we expect.\\\"\\\"\\\"\\n        # create an instance of the measure\\n        measure = MyExamplePythonModelMeasure()\\n    \\n        # make an empty model\\n        model = openstudio.model.Model()\\n    \\n        # get arguments and test that they are what we are expecting\\n        arguments = measure.arguments(model)\\n        num_args = arguments.size()\\n>       assert num_args == 2\\nE       assert 1 == 2\\n\\npython_modelmeasure/tests/test_my_example_python_model_measure.py:30: AssertionError\""
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a867e980",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(irb):3: syntax error, unexpected '\\n', expecting '='",
     "output_type": "error",
     "traceback": [
      "\u001b[31mSyntaxError\u001b[0m: (irb):3: syntax error, unexpected '\\n', expecting '='"
     ]
    }
   ],
   "source": [
    "enum {\n",
    "  Minitest,\n",
    "  BAR,\n",
    "  BAZ\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "91407ab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module JUnitType\n",
    "  MINITEST = 1\n",
    "  PYTEST = 2\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "facf2661",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(irb):1: warning: already initialized constant #<Class:0x0000556432993ec8>::JUnitType::MINITEST\n",
      "(irb):1: warning: previous definition of MINITEST was here\n",
      "(irb):2: warning: already initialized constant #<Class:0x0000556432993ec8>::JUnitType::PYTEST\n",
      "(irb):2: warning: previous definition of PYTEST was here\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       ":validate"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module JUnitType\n",
    "  MINITEST = 'minitest'\n",
    "  PYTEST = 'pytest'\n",
    "  ACCEPTED_VALUES = [MINITEST, PYTEST]\n",
    "  \n",
    "  def self.validate(v)\n",
    "    raise \"Unknow value #{v}: #{ACCEPTED_VALUES}\" unless ACCEPTED_VALUES.include?(v)\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c6ed6c9d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Unknow value caca: [\"minitest\", \"pytest\"]",
     "output_type": "error",
     "traceback": [
      "\u001b[31mRuntimeError\u001b[0m: Unknow value caca: [\"minitest\", \"pytest\"]",
      "(irb):6:in `validate'",
      "(irb):in `irb_binding'"
     ]
    }
   ],
   "source": [
    "JUnitType.validate(\"caca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3a7748",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ruby 2.7.2",
   "language": "ruby",
   "name": "ruby"
  },
  "language_info": {
   "file_extension": ".rb",
   "mimetype": "application/x-ruby",
   "name": "ruby",
   "version": "2.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
